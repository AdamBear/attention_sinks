<s> Vaswani et al. (2017) introduced the Transformers model for sequence-to-sequence (seq2seq) tasks. It achieved state-of-the-art performance on machine translation and summarization tasks by introducing attention-based recurrent neural networks (RNNs, e.g., LSTMs) to capture long-range dependencies in the input sequence.
Transformers are composed of self-attention and feed-forward layers. The self-attention layer computes a weighted sum of inputs by attending to each input at different positions, whereas the feed-forward layer applies a linear transformation to the output of the self-attention layer. In this post, we’ll explore Transformer architectures and their training.
The architecture of a vanilla Transformer is shown in Figure 1. It consists of an encoder and a decoder. The encoder takes an input sequence $x \in \mathbb{R}^{T \times D}$ and outputs a sequence of hidden states $h_1, h_2, \hdots, h_T \in \mathbb{R}^{D}$. The decoder takes the hidden states of the encoder and outputs a sequence of hidden states $d_1, d_2, \hdots, d_T \in \mathbb{R}^{D}$.
Figure 1: Architecture of a vanilla Transformer.
The encoder is a stack of $N$ identical layers, each of which consists of a multi-head self-attention layer and a feed-forward layer. The multi-head self-attention layer has $K$ heads, each of which computes a weighted sum of the input sequence using different sets of keys and/or values. The feed-forward layer applies a linear transformation to the output of the self-attention layer.
The decoder is a stack of $N$ identical layers, each of which consists of a multi-head self-attention layer and a feed-forward layer. The multi-head self-attention layer has $K$ heads, each of which computes a weighted sum of the input sequence using different sets of keys and/or values. The feed-forward layer applies a linear transformation to the output of the self-attention layer.
The input sequence is fed into the encoder and the output sequence is fed into the decoder. The two sequences are concatenated and fed into the final feed-forward layer to compute the output sequence.
Training a Transformer
The objective function of a Transformer is to minimize the cross-entropy loss between the ground truth and the predicted output sequence. The cross-entropy loss is computed as follows:
$$\begin{align} L &= - \sum_{i=1}^{T} \log p(d_i | x) \\ &= - \sum_{i=1}^{T} \log \frac{e^{v_i}}{\sum_{j=1}^{D} e^{v_j}} \end{align}$$
where $p(d_i | x)$ is the probability of the $i$-th element of the output sequence being $d_i$, $v_i$ is the $i$-th element of the output sequence, and $e^{v_i}$ is the element-wise exponential of $v_i$.
During training, the encoder and decoder are jointly trained by backpropagating the gradients of the cross-entropy loss through the network. This is known as end-to-end training.
Vaswani, A., Uszkoreit, J., Gomez, A. N., Lopez, Cañibano, A., Matena, J., … Devlin, J. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762.
Khandelwal, S., Choudhury, B., Srikanta Reddy, K., & Gupta, A. (2018). Neural Turing Machines: A Survey. IEEE Transactions on Neural Networks and Learning Systems, 29(12), 3606–3621. https://doi.org/10.1109/TNNLS.2018.2860041
Bengio, y., Courville, A., Fournier-Viger, P., Grangier, D., Hénríquez, N., Léonard, J., … Rousseau, D. (2015). Representation Learning: A Review and New Perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(8), 1890–1917. https://doiforce.acm.org/10.1109/TPAMI.2014.2324898
Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation, 9(8), 1735–1780. https://doi.pionline.org/10.1162/neco.1997.9.8.1735
Graves, A., Wayne, G., Danihelka, I., & Gulcehre, C. (2014). Speech Recognition with Deep Neural Networks. ICASSP, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 6735–6743. https://doi.org/10.1109/ICASSP.2014.6858894
Krizhevsky, A., Sutskever, I., & Hinton, G. (2012). Imagenet Classification with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems, 25, 1097–1105. https://doi.org/10.1145/2383536.2383692
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Hubbard, W., Jackel, L. D., … Wojna, Z. (1989). Handwritten Digit Recognition Using a Back-Propagation Network. Neural Computation, 1(4), 541–551. https://doi.pionline.org/10.1162/neco.1989.1.4.541
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., … Rabinovich, A. (2015). Going Deeper with Convolutions. Proceedings of the IEEE conference on computer vision and pattern recognition, 1–9. https://doi.org/10.1109/CVPR.2015.7299984
Bahdanau, D., Cho, K., Bougares, F., Schwenk, H., Escorcia Romero, J., Joulin, A., … Ammar, W. (2014). Towards End-to-End Speech Recognition. In International Conference on Spoken Language Processing (ICSLP), 1533–1538. https://doi.org/10.21437/Interspeech.2014-1201
Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K. Q., Welling, M., Cross-Grove, A., … Tancik, R. (2017). Densely Connected Convolutional Networks. In International Conference on Learning Representations (ICLR), 1–10. https://openreview.net/forum?id=BWwD5b3JpbmF0aW9uX3R5cGU9dHJ1ZS5jb20=.
Han, J., Mao, Z., Zhao, X., Liu, Z., and He, K. (2015). Deep Learning in Natural Language Processing. Foundations and Trends in Information Retrieval, 9(3–4), 207–273. https://doi.org/10.1561/2400000027/
Sennrich, R., Haddow, B., Birch, A., Bradbury, J., and Nagelmeier, M. (2016). Neural Semantic Parsing. In 2016 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), 1621–163
